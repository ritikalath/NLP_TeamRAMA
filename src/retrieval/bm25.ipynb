{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IffO5-5B5r0B",
        "outputId": "d6a8f594-f7cc-4634-e571-7ce6ddcdc71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytrec_eval in /usr/local/lib/python3.9/dist-packages (0.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.9/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rank_bm25) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ast\n",
            "  Using cached AST-0.0.2.tar.gz (19 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install pytrec_eval\n",
        "!pip install rank_bm25\n",
        "!pip install ast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aliannejadi/ClariQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6pnowXL50PK",
        "outputId": "d3b9ca5d-d1a7-43b9-8b1d-2c7a618fc80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ClariQ' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statistics import mean\n",
        "import os \n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def stem_tokenize(text, remove_stopwords=True):\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens = [word for sent in nltk.sent_tokenize(text) \\\n",
        "                                      for word in nltk.word_tokenize(sent)]\n",
        "  tokens = [word for word in tokens if word not in \\\n",
        "          nltk.corpus.stopwords.words('english')]\n",
        "  return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "logging.basicConfig(\n",
        "  level=logging.INFO,\n",
        "  format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "  handlers=[\n",
        "      logging.StreamHandler(sys.stdout)\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHcUAfUd53Tc",
        "outputId": "67e795d2-0b21-4d6e-d3aa-ac4b9c832e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#selecting part of data for dev set for testing\n",
        "datapath = '/content/ClariQ/data/dev.tsv'\n",
        "df_dev = pd.read_csv(datapath, sep='\\t')\n",
        "\n",
        "dev = df_dev.drop(columns=['clarification_need'])\n",
        "dev = dev[~dev[\"question\"].isnull()]\n",
        "\n",
        "#preprocessing question bank for BM25\n",
        "question_bank_path = '/content/ClariQ/data/question_bank.tsv'\n",
        "question_bank = pd.read_csv(question_bank_path,sep='\\t').fillna('')\n",
        "question_bank['tokenized_question_list'] = question_bank['question'].map(stem_tokenize)\n",
        "question_bank['tokenized_question_str'] = question_bank['tokenized_question_list'].map(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "vaEZrjbu556D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_file_path = 'dev_bm25'\n",
        "\n",
        "bm25_corpus = question_bank['tokenized_question_list'].tolist()\n",
        "bm25 = BM25Okapi(bm25_corpus)\n",
        "rerank_top_k = 30\n",
        "with open(run_file_path, 'w') as fo:\n",
        "  for tid in df_dev['topic_id'].unique():\n",
        "    query = df_dev.loc[df_dev['topic_id']==tid, 'initial_request'].tolist()[0]\n",
        "    bm25_ranked_list = bm25.get_top_n(stem_tokenize(query, True), \n",
        "                                    bm25_corpus, \n",
        "                                    n=30)\n",
        "    bm25_q_list = [' '.join(sent) for sent in bm25_ranked_list]\n",
        "    docs = question_bank.set_index('tokenized_question_str').loc[bm25_q_list, 'question'].tolist()\n",
        "    preds = question_bank.set_index('tokenized_question_str').loc[bm25_q_list, 'question_id'].tolist()\n",
        "   \n",
        "    for i, qid in enumerate(preds):    \n",
        "      fo.write('{} 0 {} {} {} bm25\\n'.format(tid, qid, i, len(preds)-i))"
      ],
      "metadata": {
        "id": "4NqghZRV58lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    topic_df = df_dev\n",
        "    run_file = run_file_path\n",
        "    topic_question_set_dict = topic_df.groupby('topic_id')['question_id'].agg(set).to_dict()\n",
        "    run_df = pd.read_csv(run_file, sep=' ', header=None)\n",
        "    run_df = run_df.sort_values(by=[0, 4], ascending=False).drop_duplicates(subset=[0, 4], keep='first')\n",
        "    run_question_set_list = run_df.groupby(0)[2].agg(list).to_dict()\n",
        "    topk_list = [5, 10, 20, 30]\n",
        "    recall_score_dict = {}\n",
        "    for topk in topk_list:\n",
        "        metric_name = 'Recall{}'.format(topk)\n",
        "        recall_score_dict[metric_name] = {}\n",
        "        for tid in topic_question_set_dict:\n",
        "            rec = len(set(run_question_set_list[tid][:topk]) & topic_question_set_dict[tid]) / len(\n",
        "                    topic_question_set_dict[tid])\n",
        "            recall_score_dict[metric_name][tid] = rec\n",
        "\n",
        "\n",
        "    mean_performance = {}\n",
        "    for metric in recall_score_dict:\n",
        "        mean_performance[metric] = mean(recall_score_dict[metric][k] for k in recall_score_dict[metric])\n",
        "\n",
        "    for metric in recall_score_dict:\n",
        "        print('{}: {}'.format(metric, mean_performance[metric]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdIOQYWP5_R3",
        "outputId": "fd2ff78d-b884-46e3-df12-8cca39baeeb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall5: 0.3245570421150917\n",
            "Recall10: 0.5638042646208281\n",
            "Recall20: 0.6674997108155003\n",
            "Recall30: 0.6912818698329535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  import pickle\n",
        "  topic_df = df_dev\n",
        "  # print(topic_df)\n",
        "  context_array = topic_df['facet_id'].values\n",
        "\n",
        "  eval_file_path = '/content/ClariQ/data/single_turn_train_eval.pkl'\n",
        "  with open(eval_file_path, 'rb') as fi:\n",
        "            eval_dict = pickle.load(fi)\n",
        "\n",
        "  new_eval_dict = {}\n",
        "  for metric in eval_dict:\n",
        "      new_eval_dict[metric] = {}\n",
        "      for fid in eval_dict[metric]:\n",
        "          if fid in context_array:\n",
        "                new_eval_dict[metric][fid] = eval_dict[metric][fid]\n",
        "\n",
        "  eval_dict = new_eval_dict\n",
        "  \n",
        "  run_df = pd.read_csv(run_file, sep=' ', header=None).fillna('')\n",
        "  run_df = run_df.sort_values(by=4).drop_duplicates(subset=[0], keep='last')  # we only keep the top ranked question.\n",
        "  run_dict = run_df.set_index(0)[2].to_dict()\n",
        "  \n",
        "  facet_to_topic_dict = topic_df.set_index('facet_id')['topic_id'].to_dict()\n",
        "\n",
        "  performance_dict = {}\n",
        "  for metric in eval_dict:\n",
        "        performance_dict[metric] = {}\n",
        "        for context_id in eval_dict[metric]:\n",
        "            try:\n",
        "              selected_q = run_dict[facet_to_topic_dict[context_id]]\n",
        "              selected_q = 'MIN' if selected_q == 'MAX' else selected_q \n",
        "              \n",
        "              try:\n",
        "                performance_dict[metric][context_id] = eval_dict[metric][context_id][selected_q]['with_answer']\n",
        "              except KeyError:  # if question is not among candidate question, we consider it equal to minimum performance.\n",
        "                performance_dict[metric][context_id] = eval_dict[metric][context_id]['MIN']['with_answer']\n",
        "            except KeyError:  # if there is no prediction provided for a facet, we consider performance 0.\n",
        "              performance_dict[metric][context_id] = 0.\n",
        "  \n",
        "\n",
        "    # compute the mean performance per metric and print\n",
        "  mean_performance = {}\n",
        "  for metric in performance_dict:\n",
        "        mean_performance[metric] = mean(performance_dict[metric][k] for k in performance_dict[metric])"
      ],
      "metadata": {
        "id": "nnwi0Vr16QKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " for metric in performance_dict:\n",
        "            print('{}: {}'.format(metric, mean_performance[metric]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnMdm9oyYwei",
        "outputId": "0f8cde31-63d6-4efa-bd6c-a98a15b04366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG1: 0.1859375\n",
            "NDCG3: 0.16076878817226317\n",
            "NDCG5: 0.15299655291242895\n",
            "NDCG10: 0.13569348827493657\n",
            "NDCG20: 0.12814916594090767\n",
            "P1: 0.23125\n",
            "P3: 0.18958333333333333\n",
            "P5: 0.17500000000000002\n",
            "P10: 0.139375\n",
            "P20: 0.1178125\n",
            "MRR100: 0.3090313834647762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZol7EgLdEn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}